

Evaluation/Future Work:
-----------------------

One thing that we noticed is that as they trained more, the networks developed a greedy approach where they tried to win by repeatedly stacking in a single column, or trying to win with a horizontal line. Although, this approach is very fruitful against random players (expected win percentage of ~80%), it is also a very easy strategy to defeat and come out ahead. Thus, one would expect that two networks when pitted against one another, would learn to avoid this loss, and then also learn a more solid strategy to use instead.

That this did not occur could have a number of explanations. 

A first reason that this was the strategy the networks used is that, with random play, horizontal and vertical wins are far more common than diagonal ones, and the networks could consequently not develop the diagonal filters as well as the straight ones. The main problem here, then, is that the number of games played was simply too low. This idea was, however, not supported by the fact that the performance did not increase over time. Rather, it either stabilised near the peak, or fell in performance first only to stabilize there. This of course is no definite proof of our claim, but the number of games seems to be unlikely to be limiting factor.

Secondly, the noted 80% winrate versus random players indicates that this strategy leads to a local minimum. If alternative strategies tend to punish more than the exploration is worth, an agent would likely stay in that minimum.

To that end we built in an approach to exploration that made only random moves initially, then gradually started relying more on what it had learned (i.e. lowering the exploration rate). We did this in an attempt to break out of any local minima.

... Something about the results of 'super-exploration'...

